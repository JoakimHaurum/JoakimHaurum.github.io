<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Joakim Bruslund Haurum</title>
  
  <meta name="author" content="Joakim Bruslund Haurum">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Joakim Bruslund Haurum</name>
              </p>
              <p>I am a Postdoctoral Researcher in the <a href="https://vap.aau.dk/">Visual Analysis and Perception Laboratory</a> at <a href="https://www.en.aau.dk/">Aalborg University</a>, where I work on computer vision and fine-grained visual categorization. I'm also an affiliated member of the <a href="https://www.aicentre.dk/">Pioneer Centre for Artificial Intelligence</a> and an <a href="https://ellis.eu/members">ELLIS Member</a>. Previously, I was a member of the Young Academy Panel at the <a href="https://ddsa.dk/">Danish Data Science Academy</a>.
              </p>
              <p>
                During my Ph.D. I worked on Computer Vision Aided Sewer Inspections and Marine Vision, advised by Professor <a href="https://www.create.aau.dk/tbm">Thomas B. Moeslund</a>. 
                I have previously visited the <a href="http://www.cvc.uab.es/research-lines/hupba/">Human Pose Recovery and Behavior Analysis</a> in 2021 working with Professor <a href="https://sergioescalera.com/">Sergio Escalera</a>, been a visiting researcher at the <a href="https://vectorinstitute.ai/">Vector Institute</a> and the Machine Learning Research Group at the University of Guelph in 2023 working with Professor <a href="https://www.gwtaylor.ca/">Graham Taylor</a>, and in 2024 I was a visiting researcher at the University of Edinburgh working together with Reader <a href="https://homepages.inf.ed.ac.uk/omacaod//">Oisin Mac Aodha</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:joha@create.aau.dk">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=GAEtgr4AAAAJ&hl">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/JoakimHaurum/">Github</a> &nbsp/&nbsp
                <a href="https://huggingface.co/joakimbh">HuggingFace</a>
                <a href="https://twitter.com/jbhaurum">Twitter</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/joakimhaurum/">LinkedIn</a> &nbsp/&nbsp
                <a href="images/JBH_ProfilePic.jpg">Headshot</a> <!-- &nbsp/&nbsp
                  <a href="data/JonBarron-CV.pdf">CV</a>  -->
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/JBH_ProfilePic.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/JBH_ProfilePic_circle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading>What's New</heading>
          <br>
            <tr>
              <td><strong>[Sep 2024]</strong></td>
              <td>Back from my paternity leave. While I was gone our ATC paper was accepted at ECCV and I become an ELLIS member!</td>
            </tr>
            <tr>
              <td><strong>[May 2024]</strong></td>
              <td>Back from my research stay in Edinburgh and now on 3 month parental leave. See you in September!</td>
            </tr>
            <tr>
              <td><strong>[Feb 2024]</strong></td>
              <td>Started as a Visiting Researcher at the University of Edinburgh working with Lecturer Oisin Mac Aodha for the next 3.5 months!</td>
            </tr>
            <tr>
              <td><strong>[Sep 2023]</strong></td>
              <td>Got our BIOSCAN-1M Insect Dataset accepted at NeurIPS 2023!</td>
            </tr>
            <tr>
              <td><strong>[Aug 2023]</strong></td>
              <td>Our in-depth analysis of Token Reduction methods in ViTs is accepted at the "New Ideas in Vision Transformers" ICCV workshop.</td>
            </tr>
            <tr>
              <td><strong>[Apr 2023]</strong></td>
              <td>Gave a talk at York University. Thank you for the invite <a href="https://csprofkgd.github.io/">Kosta</a>!</td>
            </tr>
            <tr>
              <td><strong>[Feb 2023]</strong></td>
              <td>Presented a poster at the Vector Institute Research Symposium 2023.</td>
            </tr>
            <tr>              
              <td><strong>[Jan 2023]</strong></td>
              <td>Started as a Research Intern at the Vector Institute working with Prof. Graham Taylor for the next 3 months. </td>
            </tr>
            <tr>              
                <td><strong>[Nov 2022]</strong></td>
                <td>Attended <a href="https://ddsa.dk/danishdatascience2022/">DDS</a> and co-hosted the MLOps and Reproducible AI parallel session.</td>
            </tr>
            <tr>              
                <td><strong>[Oct 2022]</strong></td>
                <td>Presented MOTCOM at ECCV and hosted the CVCIE workshop.</td>
            </tr>
            <tr>              
                <td><strong>[Sep 2022]</strong></td>
                <td>Started as Post Doc. in VAP lab with funding from the Pioneer Centre for AI. </td>
            </tr>
            <tr>              
                <td><strong>[Jul 2022]</strong></td>
                <td>Attended ICVSS in sunny Sicily!</td>
            </tr>
            <tr>              
                <td><strong>[Jun 2022]</strong></td>
                <td>Attended CVPR and co-hosted the CVSports workshop.</td>
            </tr>
            <tr>              
                <td><strong>[Jun 2022]</strong></td>
                <td>Defended my Ph.D.! Many thanks to my committee for a great defence.  </td>
            </tr>
            <tr>              
                <td><strong>[Apr 2022]</strong></td>
                <td>Submitted my Ph.D. thesis.</td>
            </tr>
            <tr>              
                <td><strong>[Mar 2022]</strong></td>
                <td>Attended the opening of the Pioneer Centre for AI and co-hosted the Workshop on Climate and Conservation.</td>
            </tr>
            <tr>              
                <td><strong>[Jan 2022]</strong></td>
                <td>Presented our recent work on mutli-task classification at WACV.</td>
            </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in computer vision, machine learning, marine vision, fine-grained visual categorization, and real-life applications. Representative papers are <span class="highlight">highlighted</span>.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='images/ZSC_teaser.JPG' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="http://scottclowe.com/zs-ssl-clustering/">
                <papertitle>An Empirical Study into Clustering of Unseen Datasets with Self-Supervised Encoders</papertitle>
              </a>
              <br>
              Scott C. Lowe*,
              <strong>Joakim Bruslund Haurum</strong>*,
              Sageev Oore**,
              Thomas B. Moeslund**,
              Graham W. Taylor**
              <br>
              <em>Under Review</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2406.02465">arXiv</a>
              /
              <a href="https://github.com/scottclowe/zs-ssl-clustering/">code</a>
              /
              <a href="https://sslneurips23.github.io/paper_pdfs/paper_70.pdf">NeurIPS SSL Workshop</a>
              /
              <a href="https://openreview.net/pdf?id=2gytoWpJGf">NeurIPS R0-FOMO Workshop</a>
              /
              <a href="https://openreview.net/pdf?id=kcgf3istoO">ICML FM-Wild Workshop</a>
              <p></p>
              <p>
                We explore whether pretrained models can provide a useful representation space for datasets they were not trained on for the purpose of grouping novel unlabelled data into meaningful clusters. 
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='images/neurips23_bioscan.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://biodiversitygenomics.net/projects/5m-insects/">
                <papertitle>BIOSCAN-5M: A Multimodal Dataset for Insect Biodiversity</papertitle>
              </a>
              <br>
              Zahra Gharaee*,
              Scott C. Lowe*,
              ZeMing Gong*,
              Pablo Millan Arias*,
              Nicholas Pellegrino,
              Austin T. Wang,
              <strong>Joakim Bruslund Haurum</strong>,
              Iuliia Zarubiieva,
              Lila Kari,
              Dirk Steinke**,
              Graham W. Taylor**,
              Paul Fieguth**,
              Angel X. Chang**
              <br>
              <em>Under Review</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2406.12723">arXiv</a>
              /
              <a href="https://github.com/zahrag/BIOSCAN-5M">code</a>
              <p></p>
              <p>
                We construct a large-scale fine-grained dataset of the Insect class with 5M data samples, each containing a biological taxonomic annotation, DNA barcode sequence, geographical location, and RGB image.
              </p>
            </td>
          </tr>  		


          <tr>
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='images/BIOSCANCLIP_teaser.JPG' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://3dlg-hcvc.github.io/bioscan-clip/">
                <papertitle>BIOSCAN-CLIP: Bridging Vision and Genomics for Biodiversity Monitoring at Scale</papertitle>
              </a>
              <br>
              ZeMing Gong,
              Austin Wang,
              <strong>Joakim Bruslund Haurum</strong>,
              Scott C. Lowe,
              Graham W. Taylor,
              Angel X. Chang
              <br>
              <em>Under Review</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2405.17537">arXiv</a>
              /
              <a href="https://github.com/3dlg-hcvc/bioscan-clip">code</a>
              /
              <a href="https://drive.google.com/file/d/1S4B9eaUmPPJec7pENFov3_RjryQh-GdX/view?usp=drive_link">CVPR FGVC Workshop</a>
              <p></p>
              <p>
                We introduce BIOSCAN-CLIP, the first approach to use contrastive learning for aligning biological images with DNA barcodes and taxonomic labels to enhance taxonomic classification.
              </p>
            </td>
          </tr>

          
          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='images/atc_teaser.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://vap.aau.dk/atc/">
                <papertitle>Agglomerative Token Clustering</papertitle>
              </a>
              <br>
              <strong>Joakim Bruslund Haurum</strong>,
              Sergio Escalera,
              Graham W. Taylor,
              Thomas B. Moeslund
              <br>
              <em>ECCV</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2409.11923">arXiv</a>
              /
              <a href="https://github.com/JoakimHaurum/ATC">code</a>
              /
              <a href="https://huggingface.co/joakimbh/ATC">models</a>
              /
              <a href="data/ECCV2024.bib">bibtex</a>
              <p></p>
              <p>
                We present Agglomerative Token Clustering (ATC), a novel token merging method that consistently outperforms previous token merging and pruning methods across image classification, image synthesis, and object detection & segmentation tasks. 
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='images/cvpr24_gs_teaser.JPG' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://openreview.net/pdf?id=cesQERAXhi">
                <papertitle>From NeRF to 3DGS: A Leap in Stereo Dataset Quality?</papertitle>
              </a>
              <br>
              Magnus Kaufmann Gjerde,
              Filip Slez&#225k,
              <strong>Joakim Bruslund Haurum</strong>,
              Thomas B. Moeslund
              <br>
              <em>CVPR Workshops</em>, 2024
              <br>
              <a href="data/CVPR2024_GS.bib">bibtex</a>
              <p></p>
              <p>
                We investigate whether using 3D Gaussian Splatting (3DGS) instead of NeRFs to produce stereo camera views leads to better dense disparity labels.
              </p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='images/icmva24_teaser.JPG' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://dl.acm.org/doi/abs/10.1145/3653946.3653970">
                <papertitle>Enhancing Direct Visual Odometry with Deblurring and Saliency Maps</papertitle>
              </a>
              <br>
              Magnus Kaufmann Gjerde,
              Kamal Nasrollahi,
              Thomas B. Moeslund,
              <strong>Joakim Bruslund Haurum</strong>
              <br>
              <em>ICMVA</em>, 2024
              <br>
              <a href="data/ICMVA2024.bib">bibtex</a>
              <p></p>
              <p>
                We investigate the effect of integrating a deblurring module with a saliency predictor to perform better point sampling for direct visual odometry. 
              </p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='images/wacv24_teaser.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://openaccess.thecvf.com/content/WACV2024/papers/Gaarsdal_AssemblyNet_A_Point_Cloud_Dataset_and_Benchmark_for_Predicting_Part_WACV_2024_paper.pdf">
                <papertitle>AssemblyNet: A Point Cloud Dataset and Benchmark for Predicting Part Directions in an Exploded Layout</papertitle>
              </a>
              <br>
              Jesper Gaarsdal*,
              <strong>Joakim Bruslund Haurum</strong>*,
              Sune Wolff,
              Claus Br√∏ndgaard Madsen
              <br>
              <em>WACV</em>, 2024
              <br>
              <a href="https://github.com/jgaarsdal/AssemblyNet">code</a>
              /
              <a href="data/WACV2024.bib">bibtex</a>
              <p></p>
              <p>
                We propose AssemblyNet, a novel dataset for predicting part directions in assembly models for exploded view visuzalitions. We propose a novel two-path network for predicting part directions.
              </p>
            </td>
          </tr> 
          <tr>
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='images/barcode_teaser.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://arxiv.org/abs/2311.02401v1">
                <papertitle>BarcodeBERT: Transformers for Biodiversity Analysis</papertitle>
              </a>
              <br>
              Pablo Millan Arias*,
              Niousha Sadjadi*,
              Monireh Safari*,
              ZeMing Gong**,
              Austin T. Wang**,
              Scott C. Lowe,
              <strong>Joakim Bruslund Haurum</strong>,
              Iuliia Zarubiieva,
              Dirk Steinke,
              Lila Kari,
              Angel X. Chang,
              Graham W. Taylor
              <br>
              <em>NeurIPS SSL Workshop</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2311.02401v1">arXiv</a>
              /
              <a href="https://github.com/kari-genomics-lab/barcodebert">code</a>
              /
              <a href="data/NeurIPSW2023_Barcode.bib">bibtex</a>
              <p></p>
              <p>
                We propose BarcodeBERT, the first self-supervised method for general biodiversity analysis and highlight how the role of self-supervised pretraining in achieving high-accuracy DNA barcode-based identification at the species and genus level.
              </p>
            </td>
          </tr> 
          <tr>
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='images/neurips23_bioscan.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://biodiversitygenomics.net/1M_insects/">
                <papertitle>A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset</papertitle>
              </a>
              <br>
              Zahra Gharaee*,
              ZeMing Gong*,
              Nicholas Pellegrino*,
              Iuliia Zarubiieva,
              <strong>Joakim Bruslund Haurum</strong>,
              Scott C. Lowe,
              Jaclyn T.A. McKeown,
              Chris C.Y. Ho,
              Joschka McLeod,
              Yi-Yun C Wei,
              Jireh Agda,
              Sujeevan Ratnasingham,
              Dirk Steinke**,
              Angel X. Chang**,
              Graham W. Taylor**,
              Paul Fieguth**
              <br>
              <em>NeurIPS (D&B Track)</em>, 2023
              <br>
              <a href="https://biodiversitygenomics.net/1M_insects/">project page</a>
              /
              <a href="https://arxiv.org/abs/2307.10455">arXiv</a>
              /
              <a href="https://github.com/zahrag/BIOSCAN-1M">code</a>
              /
              <a href="data/NeurIPS2023.bib">bibtex</a>
              <p></p>
              <p>
                We construct a large-scale fine-grained dataset of the Insect class with 1M data samples, each containing a biological taxonomic annotation, DNA barcode sequence, and RGB image.
              </p>
            </td>
          </tr>  				
          
          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='images/iccvw23_tokenreduc.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://vap.aau.dk/tokens/">
                <papertitle>Which Tokens to Use? Investigating Token Reduction in Vision Transformers</papertitle>
              </a>
              <br>
              <strong>Joakim Bruslund Haurum</strong>,
              Sergio Escalera,
              Graham W. Taylor,
              Thomas B. Moeslund
              <br>
              <em>ICCV Workshops</em>, 2023
              <br>
              <a href="https://vap.aau.dk/tokens/">project page</a>
              /
              <a href="https://arxiv.org/abs/2308.04657">arXiv</a>
              /
              <a href="https://github.com/JoakimHaurum/TokenReduction">code</a>
              /
              <a href="data/ICCVW2023.bib">bibtex</a>
              <p></p>
              <p>
                We conduct the first systematic comparison and analysis of 10 state-of-the-art token reduction methods across four image classification datasets, trained using a single codebase and consistent training protocol.
              </p>
            </td>
          </tr>   
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='images/aic2022_teaser.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://vap.aau.dk/mshvit/">
                <papertitle>Multi-scale hybrid vision transformer and Sinkhorn tokenizer for sewer defect classification</papertitle>
              </a>
              <br>
              <strong>Joakim Bruslund Haurum</strong>,
              Meysam Madadi,
              Sergio Escalera,
              Thomas B. Moeslund
              <br>
              <em>AiC</em>, 2022
              <br>
              <a href="https://vap.aau.dk/mshvit/">project page</a>
              /
              <a href="https://github.com/JoakimHaurum/MSHViT">code</a>
              /
              <a href="data/AiC2022.bib">bibtex</a>
              <p></p>
              <p>
                We propose a novel method for to model spatial semantics in images, where features are aggregated at different scales non-locally using a lightweight vision transformer, and novel Sinkhorn clustering-based tokenizer. 
              </p>
            </td>
          </tr>      

          
          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='images/motcom_teaser.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://vap.aau.dk/motcom/">
                <papertitle>MOTCOM: The Multi-Object Tracking Dataset Complexity Metric</papertitle>
              </a>
              <br>
              Malte Pedersen,
              <strong>Joakim Bruslund Haurum</strong>,
              Patrick Dendorfer,
              Thomas B. Moeslund
              <br>
              <em>ECCV</em>, 2022
              <br>
              <a href="https://vap.aau.dk/motcom/">project page</a>
              /
              <a href="https://arxiv.org/abs/2207.10031">arXiv</a>
              /
              <a href="https://github.com/JoakimHaurum/MOTCOM">code</a>
              /
              <a href="data/MOTCOM2022.bib">bibtex</a>
              <p></p>
              <p>
                We propose a set of metrics to determine the complexity of Multi-Object Tracking datasets along three axes: Motion, Appearance, and Occlusion.
              </p>
            </td>
          </tr>      
          
          
          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='images/asir_teaser.JPG' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://vbn.aau.dk/ws/portalfiles/portal/478559005/phd_JBH_e_pdf.pdf">
                <papertitle>A Deep Dive into Computer Vision Aided Sewer Inspections</papertitle>
              </a>
              <br>
              <strong>Joakim Bruslund Haurum</strong>
              <br>
              <em>Ph.D. Thesis</em>, 2022
              <br>
              <a href="data/Thesis2022.bib">bibtex</a>
              <p></p>
              <p>
                My Ph.D. thesis summarizing my work on computer vision aided sewer inspections. 
              Supervised by <a href="https://www.create.aau.dk/tbm">Thomas B. Moeslund</a>. 
              </p>
              <p>
              Assessment committe:  <a href="http://homes.create.aau.dk/gt/">Georgios A. Triantafyllidi</a> (chair.), <a href="https://sergebelongie.github.io/">Serge Belongie</a>, and <a href="https://www.gwtaylor.ca/">Graham W. Taylor</a>.
              </p>
            </td>
          </tr>      
  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='images/nldl_teaser.JPG' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://septentrio.uit.no/index.php/nldl/article/view/6234/6504">
                <papertitle>Re-Identification of Giant Sunfish using Keypoint Matching</papertitle>
              </a>
              <br>
              Malte Pedersen,
              <strong>Joakim Bruslund Haurum</strong>,
              Thomas B. Moeslund,
              Marianne Nyegaard
              <br>
              <em>NLDL</em>, 2022
              <br>
              <a href="data/NLDL2022.bib">bibtex</a>
              <p></p>
              <p>
                We propose a keypoint matching based framework for re-identification of Giant Sunfish, comparing SIFT, RootSIFT, and SuperPoint descriptors.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='images/wacv22_teaser.JPG' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://vap.aau.dk/ctgnn/">
                <papertitle>Multi-Task Classification of Sewer Pipe Defects and Properties using a Cross-Task
                  Graph Neural Network Decoder</papertitle>
              </a>
              <br>
              <strong>Joakim Bruslund Haurum</strong>,
              Meysam Madadi,
              Sergio Escalera,
              Thomas B. Moeslund
              <br>
              <em>WACV</em>, 2022
              <br>
              <a href="https://vap.aau.dk/ctgnn/">project page</a>
              /
              <a href="https://arxiv.org/abs/2111.07846">arXiv</a>
              /
              <a href="https://bitbucket.org/aauvap/ctgnn">code</a>
              /
              <a href="data/WACV2022.bib">bibtex</a>
              <p></p>
              <p>
                We propose a novel decoder-focused multi-task classification architecture called Cross-Task Graph Neural Network (CT-GNN), which refines the disjointed per-task predictions using cross-task information.
              </p>
            </td>
          </tr>
          
          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='images/cvpr21_teaser.JPG' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://vap.aau.dk/sewer-ml/">
                <papertitle>Sewer-ML: A Multi-Label Sewer Defect Classification Dataset and Benchmark</papertitle>
              </a>
              <br>
              <strong>Joakim Bruslund Haurum</strong>,
              Thomas B. Moeslund
              <br>
              <em>CVPR</em>, 2021
              <br>
              <a href="https://vap.aau.dk/sewer-ml/">project page</a>
              /
              <a href="https://arxiv.org/abs/2103.10895">arXiv</a>
              /
              <a href="https://bitbucket.org/aauvap/sewer-ml">code</a>
              /
              <a href="https://forms.gle/hBaPtoweZumZAi4u9">dataset</a>
              /
              <a href="data/CVPR2021.bib">bibtex</a>
              <p></p>
              <p>
                We present the world's first open-source sewer defect dataset, with multi-label annotations and 1.3 million images from Danish sewers.
              </p>
            </td>
          </tr>

          <tr >
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='images/visapp_teaser.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://www.scitepress.org/PublicationsDetail.aspx?ID=TjPZq1xHMEQ=&t=1">
                <papertitle>Sewer Defect Classification using Synthetic Point Clouds</papertitle>
              </a>
              <br>
              <strong>Joakim Bruslund Haurum</strong>,
              Moaaz M. J. Allahham,
              Mathias S. Lynge,
              Kasper Sch√∏n Henriksen,
              Ivan A. Nikolov, 
              Thomas B. Moeslund
              <br>
              <em>VISAPP</em>, 2021
              <br>
              <a href="https://bitbucket.org/aauvap/sewer3dclassification">code</a>
              /
              <a href="https://www.kaggle.com/datasets/aalborguniversity/sewerpointclouds">dataset</a>
              /
              <a href="data/VISAPP2021.bib">bibtex</a>
              <p></p>
              <p>
                Investigation into comibning synthetic and real point cloud data of sewer pipes for sewer defect classification using PointNet and DGCNN.
              </p>
            </td>
          </tr>

          <tr >
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='images/water_teaser.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://www.mdpi.com/2073-4441/12/12/3412">
                <papertitle>Water Level Estimation in Sewer Pipes using Deep Convolutional Neural Networks</papertitle>
              </a>
              <br>
              <strong>Joakim Bruslund Haurum</strong>,
              Chris H. Bahnsen,
              Malte Pedersen, 
              Thomas B. Moeslund
              <br>
              <em>Water</em>, 2020
              <br>
              <a href="https://bitbucket.org/aauvap/waterlevelestimation">code</a>
              /
              <a href="data/Water2020.bib">bibtex</a>
              <p></p>
              <p>
                We estimate the water level in sewer pipes by casting the task as a classification and regression problem. 
              </p>
            </td>
          </tr>

          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='images/cvpr20_teaser.JPG' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://vap.aau.dk/3d-zef/">
                <papertitle>3D-ZeF: A 3D Zebrafish Tracking Benchmark Dataset</papertitle>
              </a>
              <br>
              Malte Pedersen*, 
              <strong>Joakim Bruslund Haurum</strong>*,
              Stefan Hein Bengtson,
              Thomas B. Moeslund
              <br>
              <em>CVPR</em>, 2020
              <br>
              <a href="https://vap.aau.dk/3d-zef/">project page</a>
              /
              <a href="https://arxiv.org/abs/2006.08466">arXiv</a>
              /
              <a href="https://bitbucket.org/aauvap/3d-zef">code</a>
              /
              <a href="https://motchallenge.net/data/3D-ZeF20">dataset</a>
              /
              <a href="data/CVPR2020.bib">bibtex</a>
              <p></p>
              <p>
                We presenta a dataset for 3D multi-object tracking of zebrafish, captured using an off-the-shelf setup. 
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='images/avr2020_teaser.JPG' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://link.springer.com/chapter/10.1007/978-3-030-58468-9_26">
                <papertitle>Generating Synthetic Point Clouds of Sewer Networks: An Initial Investigation</papertitle>
              </a>
              <br>
              Kasper Sch√∏n Henriksen,
              Mathias S. Lynge,
              Mikkel D. B. Jeppesen,
              Moaaz M. J. Allahham,
              Ivan A. Nikolov,
              <strong>Joakim Bruslund Haurum</strong>,
              Thomas B. Moeslund
              <br>
              <em>AVR</em>, 2020
              <br>
              <a href="https://bitbucket.org/aauvap/syntheticsewerpipes">code</a>
              /
              <a href="data/AVR2020.bib">bibtex</a>
              <p></p>
              <p>
                We propose a system for generating synthetic point clouds of sewer pipes  using Structured Domain Randomization for the generation of the sewer systems and an approximated model of a Pico Flexx Time-of-Flight camera. 
              </p>
            </td>
          </tr>

          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='images/aic2020_teaser.JPG' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://www.sciencedirect.com/science/article/abs/pii/S0926580519311227">
                <papertitle>A Survey on Image-Based Automation of CCTV and SSET Sewer Inspections</papertitle>
              </a>
              <br>
              <strong>Joakim Bruslund Haurum</strong>,
              Thomas B. Moeslund
              <br>
              <em>AiC</em>, 2020
              <br>
              <a href="data/AiC2020.bib">bibtex</a>
              <p></p>
              <p>
                We reviewed 113 articles on image-based automated sewer inspection methodologies, finding that there are is a severe lack of 1) publicly available datasets, 2) commonly agreed upon evaluation protocols, and 3) open-sourced code.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='images/wacvw2020_teaser.JPG' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://openaccess.thecvf.com/content_WACVW_2020/papers/w2/Haurum_Re-Identification_of_Zebrafish_using_Metric_Learning_WACVW_2020_paper.pdf">
                <papertitle>Re-Identification of Zebrafish using Metric Learning</papertitle>
              </a>
              <br>
              <strong>Joakim Bruslund Haurum</strong>*,
              Anastasija Karpova*,
              Malte Pedersen,
              Stefan Hein Bengtson,
              Thomas B. Moeslund
              <br>
              <em>WACV Workshops</em>, 2020
              <br>
              <a href="https://bitbucket.org/aauvap/zebrafish-re-identification/src/master/">code</a>
              /
              <a href="https://www.kaggle.com/datasets/aalborguniversity/aau-zebrafish-reid">dataset</a>
              /
              <a href="data/WACVW2020.bib">bibtex</a>
              <p></p>
              <p>
                We show that it is possible to re-identify zebrafish from a sideview, using metric learning and classical feature descriptors.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='images/cvpr19_brack_teaser.JPG' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/AAMVEM/Pedersen_Detection_of_Marine_Animals_in_a_New_Underwater_Dataset_with_CVPRW_2019_paper.pdf">
                <papertitle>Detection of Marine Animals in a New Underwater Dataset with Varying Visibility</papertitle>
              </a>
              <br>
              Malte Pedersen,
              <strong>Joakim Bruslund Haurum</strong>,
              Rikke Gade,
              Thomas B. Moeslund,
              Niels Madsen
              <br>
              <em>CVPR Workshops</em>, 2019
              <br>
              <a href="https://www.kaggle.com/datasets/aalborguniversity/brackish-dataset">dataset</a>
              /
              <a href="data/CVPR2019_Brackish.bib">bibtex</a>
              <p></p>
              <p>
                We present a new publicly available underwater dataset with annotated image sequences of fish, crabs, and starfish captured in brackish water with varying visibility.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='images/cvpr19_rain_teaser.JPG' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/Vision%20for%20All%20Seasons%20Bad%20Weather%20and%20Nighttime/Haurum_Is_it_Raining_Outside__Detection_of_Rainfall_using_General-Purpose_CVPRW_2019_paper.pdf">
                <papertitle>Is it Raining Outside? Detection of Rainfall using General-Purpose Surveillance Cameras</papertitle>
              </a>
              <br>
              <strong>Joakim Bruslund Haurum</strong>,
              Chris H. Bahnsen,
              Thomas B. Moeslund
              <br>
              <em>CVPR Workshops</em>, 2019
              <br>
              <a href="https://arxiv.org/abs/1908.04034">arXiv</a>
              /
              <a href="https://bitbucket.org/aauvap/aau-virada">code</a>
              /
              <a href="https://zenodo.org/record/4715681">dataset</a>
              /
              <a href="data/CVPR2019_Rain.bib">bibtex</a>
              <p></p>
              <p>
                We design a system for the detection of rainfall by the use of surveillance cameras, and compare it against the former state-of-the-art method for rain detection on our new AAU Visual Rain Dataset (VIRADA).
              </p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='images/avr2017_teaser.JPG' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://link.springer.com/chapter/10.1007/978-3-319-60922-5_19">
                <papertitle>Pixel Reprojection of 360 Degree Renderings for Small Parallax Effects</papertitle>
              </a>
              <br>
              <strong>Joakim Bruslund Haurum</strong>,
              Christian Nygaard Daugbjerg,
              P√©ter Rohoska,
              Andrea Coifman,
              Anne Juhler Hansen,
              Martin Kraus
              <br>
              <em>AVR</em>, 2017
              <br>
              <a href="data/AVR2017.bib">bibtex</a>
              <p></p>
              <p>
                We apply pixel reprojection on nine 360 degree renderings to enable 3D motion and introduce motion parallax effects, without explicit knowledge of the 3D geometry.
              </p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <img src='images/SSEA_teaser.JPG' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://esamultimedia.esa.int/docs/edu/1st_Symsposium_on_Space_Educational_Activities.pdf#page=36">
                <papertitle>AAUSAT5 - an evaluation of a student-run cubesat project</papertitle>
              </a>
              <br>
              Rasmus Gundorff S√¶derup,
              <strong>Joakim Bruslund Haurum</strong>
              <br>
              <em>SSEA</em>, 2015
              <br>
              <a href="data/SSEA2015.bib">bibtex</a>
              <p></p>
              <p>
                We evaluate and discuss the experiences acquired during the 18 month long production, qualification and testing phase of the AAUSAT5 cubesat.
              </p>
            </td>
          </tr>

          <table width="100%" align="left" border="0" cellpadding="20"><tbody>            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  This website layout was adapted from <a href="https://github.com/jonbarron/website">Jon Barron's template</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>

  </body>

</html>
